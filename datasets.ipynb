{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = 'allenai/pixmo-cap'\n",
    "target_dir = '/lambdafs/jacob/data/' + repo_id.replace('/', '_')\n",
    "\n",
    "snapshot_download(repo_id=repo_id, local_dir=target_dir, repo_type='dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataCentral.data_manager import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 data entries...\n",
      "Inserting data entries using batch_insert_data...\n",
      "Data insertion complete.\n",
      "Creating index on data_type...\n",
      "Index creation complete.\n",
      "Getting rows with data_type 'type1'...\n",
      "Number of rows with data_type 'type1': 10\n",
      "Index values for UUID f77af353-c3a6-4113-a9b1-fb1070ba7c4b: ['type1']\n"
     ]
    }
   ],
   "source": [
    "# Function to generate sample data entries\n",
    "def generate_data_entries(N):\n",
    "    import random\n",
    "    import json\n",
    "\n",
    "    data_sources = ['source1', 'source2', 'source3']\n",
    "    group_ids = ['group1', 'group2', 'group3']\n",
    "    data_types = ['type1', 'type2', 'type3']\n",
    "    files_list = [['file1.txt', 'file2.txt'], ['file3.txt'], ['file4.txt', 'file5.txt', 'file6.txt']]\n",
    "\n",
    "    data_entries = []\n",
    "    for _ in range(N):\n",
    "        data_source = random.choice(data_sources)\n",
    "        group_id = random.choice(group_ids)\n",
    "        data_type = random.choice(data_types)\n",
    "        data = json.dumps({'value': random.randint(1, 1000000)})\n",
    "        files = random.choice(files_list)\n",
    "        entry = {\n",
    "            'data_source': data_source,\n",
    "            'group_id': group_id,\n",
    "            'data_type': data_type,\n",
    "            'data': data,\n",
    "            'files': files\n",
    "        }\n",
    "        data_entries.append(entry)\n",
    "    return data_entries\n",
    "\n",
    "# Initialize the DataManager\n",
    "data_manager = DataManager('my_database.db')\n",
    "\n",
    "# Generate and insert data entries\n",
    "N = 1000  # Adjust N to the desired number of entries, e.g., 10000000 for 10 million\n",
    "print(f\"Generating {N} data entries...\")\n",
    "data_entries = generate_data_entries(N)\n",
    "\n",
    "print(\"Inserting data entries using batch_insert_data...\")\n",
    "data_manager.insert_data(data_entries)\n",
    "print(\"Data insertion complete.\")\n",
    "\n",
    "# Define an index function\n",
    "def index_by_data_type(row):\n",
    "    return [row['data_type']]\n",
    "\n",
    "# Create an index\n",
    "print(\"Creating index on data_type...\")\n",
    "data_manager.create_index('data_type_index', index_by_data_type)\n",
    "print(\"Index creation complete.\")\n",
    "\n",
    "# Get rows by index value\n",
    "print(\"Getting rows with data_type 'type1'...\")\n",
    "rows_with_type1 = data_manager.get_data(data_sources=[\"source2\"], group_ids=[\"group2\"], limit=10)\n",
    "print(f\"Number of rows with data_type 'type1': {len(rows_with_type1)}\")\n",
    "\n",
    "# Get index values for a specific row\n",
    "some_uuid = rows_with_type1[0]['uuid']\n",
    "index_values = data_manager.get_index_values_for_row('data_type_index', some_uuid)\n",
    "print(f\"Index values for UUID {some_uuid}: {index_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows from source1 and group1: 0\n",
      "Group 'group1' deleted.\n",
      "Index 'data_type_index' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Get data by data source and group ID\n",
    "print(\"Getting data from source1 and group1...\")\n",
    "data = data_manager.get_data(data_sources=['source1'], group_ids=['group1'], limit=10, offset=1000000)\n",
    "print(f\"Number of rows from source1 and group1: {len(data)}\")\n",
    "\n",
    "# Delete a group\n",
    "print(\"Deleting group 'group1'...\")\n",
    "data_manager.delete_group('group1')\n",
    "print(\"Group 'group1' deleted.\")\n",
    "\n",
    "# Delete an index\n",
    "print(\"Deleting index 'data_type_index'...\")\n",
    "data_manager.delete_index('data_type_index')\n",
    "print(\"Index 'data_type_index' deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No records found for group ID 'group3'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
